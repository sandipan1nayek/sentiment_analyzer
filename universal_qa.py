"""
Universal Quality Assurance System for Sentiment Analysis
Automatically ensures high relevance for ANY query without manual checking
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime
import json

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UniversalQualityAssurance:
    """
    üéØ Universal QA system that ensures high relevance for ANY query
    - Automatic entity type detection
    - Dynamic quality scoring
    - Self-correcting pipeline
    - Performance monitoring
    """
    
    def __init__(self):
        # Universal context patterns by entity type
        self.entity_contexts = {
            "person": [
                "speech", "policy", "statement", "interview", "reaction",
                "announcement", "address", "remarks", "comments"
            ],
            "company": [
                "stock", "CEO", "earnings", "product", "launch", "review",
                "quarterly", "financial", "partnership", "acquisition"
            ],
            "technology": [
                "research", "development", "application", "breakthrough",
                "innovation", "study", "advancement", "implementation"
            ],
            "place": [
                "news", "update", "situation", "development", "report",
                "analysis", "event", "policy", "government"
            ],
            "event": [
                "coverage", "highlights", "impact", "response", "reaction",
                "analysis", "outcome", "significance", "implications"
            ],
            "general": [
                "news", "update", "analysis", "report", "development"
            ]
        }
        
        # Quality thresholds for automatic filtering
        self.quality_thresholds = {
            "minimum_relevance": 0.65,      # 65% relevance minimum
            "minimum_entity_score": 0.5,    # Entity must be clearly mentioned
            "minimum_semantic_score": 0.6,  # Semantic similarity threshold
            "minimum_text_length": 25,      # Minimum content length
            "maximum_results": 100          # Maximum results to process
        }
        
        # Performance tracking
        self.performance_metrics = {
            "queries_processed": 0,
            "total_results_fetched": 0,
            "total_results_after_filter": 0,
            "average_relevance": 0.0,
            "entity_type_distribution": {}
        }
        
    def analyze_query_quality(self, df: pd.DataFrame, query: str) -> Dict:
        """
        üîç Comprehensive quality analysis for any query
        Returns quality metrics and recommendations
        """
        if df.empty:
            return {
                "status": "no_data",
                "quality_score": 0.0,
                "recommendations": ["No data found - try broader search terms"]
            }
        
        # Calculate quality metrics
        metrics = self._calculate_quality_metrics(df, query)
        
        # Generate quality report
        quality_report = {
            "status": self._determine_quality_status(metrics),
            "quality_score": metrics["overall_quality"],
            "relevance_distribution": metrics["relevance_stats"],
            "source_diversity": metrics["source_diversity"],
            "coverage_analysis": metrics["coverage_analysis"],
            "recommendations": self._generate_recommendations(metrics, query),
            "entity_analysis": metrics["entity_analysis"]
        }
        
        # Update performance tracking
        self._update_performance_metrics(metrics, query)
        
        return quality_report
    
    def _calculate_quality_metrics(self, df: pd.DataFrame, query: str) -> Dict:
        """Calculate comprehensive quality metrics"""
        metrics = {}
        
        # 1. Relevance Analysis
        if 'relevance_score' in df.columns:
            relevance_scores = df['relevance_score']
            metrics["relevance_stats"] = {
                "average": relevance_scores.mean(),
                "median": relevance_scores.median(),
                "high_relevance_percent": (relevance_scores >= 0.7).mean() * 100,
                "low_relevance_percent": (relevance_scores < 0.5).mean() * 100
            }
        else:
            metrics["relevance_stats"] = {"average": 0.5, "note": "No relevance scores available"}
        
        # 2. Source Diversity Analysis
        source_counts = df['source'].value_counts() if 'source' in df.columns else pd.Series()
        metrics["source_diversity"] = {
            "unique_sources": len(source_counts),
            "dominant_source_percent": (source_counts.iloc[0] / len(df) * 100) if len(source_counts) > 0 else 0,
            "source_distribution": source_counts.head(5).to_dict()
        }
        
        # 3. Coverage Analysis
        metrics["coverage_analysis"] = {
            "total_items": len(df),
            "date_range": self._calculate_date_range(df),
            "content_quality": self._assess_content_quality(df),
            "category_distribution": df['category'].value_counts().to_dict() if 'category' in df.columns else {}
        }
        
        # 4. Entity Analysis
        metrics["entity_analysis"] = self._analyze_entity_coverage(df, query)
        
        # 5. Overall Quality Score
        metrics["overall_quality"] = self._calculate_overall_quality(metrics)
        
        return metrics
    
    def _determine_quality_status(self, metrics: Dict) -> str:
        """Determine overall quality status"""
        overall_quality = metrics.get("overall_quality", 0.0)
        
        if overall_quality >= 0.8:
            return "excellent"
        elif overall_quality >= 0.7:
            return "good"
        elif overall_quality >= 0.6:
            return "acceptable"
        elif overall_quality >= 0.4:
            return "poor"
        else:
            return "very_poor"
    
    def _generate_recommendations(self, metrics: Dict, query: str) -> List[str]:
        """Generate actionable recommendations based on quality analysis"""
        recommendations = []
        
        # Check relevance
        relevance_avg = metrics["relevance_stats"].get("average", 0.5)
        if relevance_avg < 0.6:
            recommendations.append("üéØ Low relevance detected - consider refining search terms")
        
        # Check source diversity
        source_diversity = metrics["source_diversity"]
        if source_diversity["unique_sources"] < 3:
            recommendations.append("üì∞ Limited source diversity - try expanding data sources")
        
        if source_diversity["dominant_source_percent"] > 70:
            recommendations.append("‚öñÔ∏è One source dominates results - need better source balancing")
        
        # Check coverage
        coverage = metrics["coverage_analysis"]
        if coverage["total_items"] < 20:
            recommendations.append("üìä Small dataset - consider longer time range or broader search")
        
        # Check content quality
        if coverage["content_quality"]["average_length"] < 50:
            recommendations.append("üìù Short content detected - may lack context for analysis")
        
        # Entity-specific recommendations
        entity_analysis = metrics["entity_analysis"]
        if entity_analysis["direct_mentions_percent"] < 80:
            recommendations.append("üîç Many results don't directly mention target - improve filtering")
        
        # Success cases
        if not recommendations:
            recommendations.append("‚úÖ High quality dataset - ready for analysis!")
        
        return recommendations
    
    def _calculate_date_range(self, df: pd.DataFrame) -> Dict:
        """Calculate temporal coverage"""
        if 'timestamp' not in df.columns or df['timestamp'].isna().all():
            return {"status": "no_dates"}
        
        dates = pd.to_datetime(df['timestamp'], errors='coerce').dropna()
        if len(dates) == 0:
            return {"status": "invalid_dates"}
        
        return {
            "start_date": dates.min().strftime('%Y-%m-%d'),
            "end_date": dates.max().strftime('%Y-%m-%d'),
            "days_covered": (dates.max() - dates.min()).days,
            "temporal_distribution": "good" if (dates.max() - dates.min()).days > 1 else "limited"
        }
    
    def _assess_content_quality(self, df: pd.DataFrame) -> Dict:
        """Assess content quality metrics"""
        if 'text' not in df.columns:
            return {"status": "no_text"}
        
        text_lengths = df['text'].str.len()
        word_counts = df['text'].str.split().str.len()
        
        return {
            "average_length": text_lengths.mean(),
            "average_words": word_counts.mean(),
            "short_content_percent": (text_lengths < 30).mean() * 100,
            "substantial_content_percent": (text_lengths >= 100).mean() * 100
        }
    
    def _analyze_entity_coverage(self, df: pd.DataFrame, query: str) -> Dict:
        """Analyze how well the entity is covered"""
        if 'text' not in df.columns:
            return {"status": "no_text"}
        
        query_lower = query.lower()
        texts = df['text'].str.lower()
        
        # Direct mentions
        direct_mentions = texts.str.contains(query_lower, na=False).sum()
        direct_mentions_percent = (direct_mentions / len(df)) * 100
        
        # Phrase analysis
        exact_phrase_mentions = texts.str.contains(f'"{query_lower}"', na=False).sum()
        
        return {
            "direct_mentions": direct_mentions,
            "direct_mentions_percent": direct_mentions_percent,
            "exact_phrase_mentions": exact_phrase_mentions,
            "coverage_quality": "high" if direct_mentions_percent >= 80 else "medium" if direct_mentions_percent >= 60 else "low"
        }
    
    def _calculate_overall_quality(self, metrics: Dict) -> float:
        """Calculate weighted overall quality score"""
        # Weight different aspects
        weights = {
            "relevance": 0.4,
            "source_diversity": 0.2,
            "coverage": 0.2,
            "entity_coverage": 0.2
        }
        
        # Relevance score
        relevance_score = metrics["relevance_stats"].get("average", 0.5)
        
        # Source diversity score (higher diversity = better)
        diversity_score = min(metrics["source_diversity"]["unique_sources"] / 5, 1.0)
        
        # Coverage score (based on content quality and quantity)
        coverage_metrics = metrics["coverage_analysis"]
        content_quality = coverage_metrics.get("content_quality", {})
        coverage_score = min(
            (coverage_metrics["total_items"] / 50) * 0.5 +  # Quantity component
            (content_quality.get("average_length", 50) / 100) * 0.5,  # Quality component
            1.0
        )
        
        # Entity coverage score
        entity_score = metrics["entity_analysis"]["direct_mentions_percent"] / 100
        
        # Calculate weighted average
        overall_score = (
            weights["relevance"] * relevance_score +
            weights["source_diversity"] * diversity_score +
            weights["coverage"] * coverage_score +
            weights["entity_coverage"] * entity_score
        )
        
        return round(overall_score, 3)
    
    def _update_performance_metrics(self, metrics: Dict, query: str):
        """Update system performance tracking"""
        self.performance_metrics["queries_processed"] += 1
        self.performance_metrics["total_results_fetched"] += metrics["coverage_analysis"]["total_items"]
        
        # Update averages
        current_avg = self.performance_metrics["average_relevance"]
        new_relevance = metrics["relevance_stats"].get("average", 0.5)
        queries_count = self.performance_metrics["queries_processed"]
        
        self.performance_metrics["average_relevance"] = (
            (current_avg * (queries_count - 1) + new_relevance) / queries_count
        )
    
    def get_system_performance_report(self) -> Dict:
        """Get overall system performance metrics"""
        if self.performance_metrics["queries_processed"] == 0:
            return {"status": "no_data", "message": "No queries processed yet"}
        
        total_fetched = self.performance_metrics["total_results_fetched"]
        total_processed = self.performance_metrics["queries_processed"]
        
        return {
            "queries_processed": total_processed,
            "average_results_per_query": total_fetched / total_processed if total_processed > 0 else 0,
            "system_average_relevance": self.performance_metrics["average_relevance"],
            "performance_status": "excellent" if self.performance_metrics["average_relevance"] >= 0.7 else "good" if self.performance_metrics["average_relevance"] >= 0.6 else "needs_improvement"
        }

# Initialize global QA system
universal_qa = UniversalQualityAssurance()

def analyze_query_quality(df: pd.DataFrame, query: str) -> Dict:
    """Main function to analyze query quality"""
    return universal_qa.analyze_query_quality(df, query)

def get_performance_report() -> Dict:
    """Get system performance report"""
    return universal_qa.get_system_performance_report()

if __name__ == "__main__":
    # Test the QA system
    print("üîç Testing Universal Quality Assurance System")
    
    # Create sample test data
    test_data = pd.DataFrame({
        'text': [
            'Tesla announces new Model 3 features',
            'Elon Musk tweets about Tesla production',
            'Tesla stock rises after earnings beat',
            'Electric vehicle market grows with Tesla leading'
        ],
        'source': ['News-Reuters', 'Social-Twitter', 'News-Bloomberg', 'News-TechCrunch'],
        'relevance_score': [0.9, 0.8, 0.85, 0.75],
        'category': ['news', 'social', 'news', 'news'],
        'timestamp': pd.date_range('2025-01-01', periods=4, freq='D')
    })
    
    # Analyze quality
    quality_report = analyze_query_quality(test_data, "Tesla")
    
    print(f"Quality Status: {quality_report['status']}")
    print(f"Quality Score: {quality_report['quality_score']}")
    print(f"Recommendations: {quality_report['recommendations']}")
    
    # Get performance report
    performance = get_performance_report()
    print(f"System Performance: {performance}")
