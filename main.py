"""
Professional Sentiment Analysis Application
A production-grade sentiment analysis platform for news and social media content

Features:
- Real-time data fetching from Reddit, HackerNews, and NewsAPI
- Advanced phrase analysis with contextual insights
- Professional visualizations with stable, clear charts
- Comprehensive entity analysis and sentiment correlation
- Multi-source data integration with quality metrics
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import time
import sys
import os

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import professional modules
from final_working_system import FinalWorkingSystem
from sentiment_analysis import SentimentAnalyzer
from ner import EntityExtractor
from visualization import ProfessionalVisualizer, create_professional_metrics
from phrase_analysis import ProfessionalPhraseAnalyzer
import config
import config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="Professional Sentiment Analyzer",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Professional CSS styling
st.markdown("""
<style>
    /* Fixed sidebar width */
    .css-1d391kg {
        width: 10cm !important;
    }
    
    .main-header {
        background: linear-gradient(90deg, #1f77b4, #2ca02c);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #1f77b4;
    }
    .status-success {
        color: #28a745;
        font-weight: bold;
    }
    .status-warning {
        color: #ffc107;
        font-weight: bold;
    }
    .status-error {
        color: #dc3545;
        font-weight: bold;
    }
    .professional-sidebar {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
    }
    
    /* Animation styles */
    .spinner {
        display: inline-block;
        width: 20px;
        height: 20px;
        border: 3px solid #f3f3f3;
        border-top: 3px solid #3498db;
        border-radius: 50%;
        animation: spin 1s linear infinite;
        margin-right: 10px;
    }
    
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
    
    .fade-in {
        animation: fadeIn 0.5s ease-in;
    }
    
    @keyframes fadeIn {
        0% { opacity: 0; }
        100% { opacity: 1; }
    }
    
    /* Bigger tab text */
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
        font-size: 18px !important;
        font-weight: 600 !important;
    }
    
    /* Table text styling - make text black for better visibility */
    .stDataFrame table {
        color: #000000 !important;
    }
    
    .stDataFrame table th {
        background-color: #f8f9fa !important;
        color: #000000 !important;
        font-weight: bold !important;
    }
    
    .stDataFrame table td {
        background-color: #ffffff !important;
        color: #000000 !important;
    }
</style>
""", unsafe_allow_html=True)

class ProfessionalSentimentApp:
    """Main application class for professional sentiment analysis"""
    
    def __init__(self):
        """Initialize the professional application"""
        self.initialize_components()
        self.initialize_session_state()
    
    def initialize_components(self):
        """Initialize all professional components"""
        try:
            # Initialize final production system
            self.final_working_system = FinalWorkingSystem()
            
            # Individual components for specific features
            self.sentiment_analyzer = SentimentAnalyzer()
            self.entity_extractor = EntityExtractor()
            self.visualizer = ProfessionalVisualizer()
            self.phrase_analyzer = ProfessionalPhraseAnalyzer()
            
            logger.info("All professional components initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing components: {e}")
            st.error(f"Initialization error: {e}")
    
    def initialize_session_state(self):
        """Initialize session state variables"""
        if 'analysis_data' not in st.session_state:
            st.session_state.analysis_data = pd.DataFrame()
        if 'last_update' not in st.session_state:
            st.session_state.last_update = None
        if 'processing_status' not in st.session_state:
            st.session_state.processing_status = "Ready"
        if 'data_quality_score' not in st.session_state:
            st.session_state.data_quality_score = 0.0
        if 'phrase_insights' not in st.session_state:
            st.session_state.phrase_insights = []
        if 'show_parameters' not in st.session_state:
            st.session_state.show_parameters = False
    
    def render_header(self):
        """Render the professional header"""
        st.markdown("""
        <div class="main-header">
            <h1>üìä Sentiment Analysis Platform</h1>
            <p>Real-time sentiment analysis across News</p>
        </div>
        """, unsafe_allow_html=True)
    
    def render_professional_sidebar(self):
        """Render the professional sidebar with controls"""
        with st.sidebar:
            st.markdown('<div class="professional-sidebar">', unsafe_allow_html=True)
            
            st.header("‚öôÔ∏è Configuration")
            
            # Search configuration only
            st.subheader("Search Parameters")
            search_query = st.text_input(
                "Search keywords:",
                value="technology AI artificial intelligence",
                help="Keywords to search for in content"
            )
            
            # Set default values (no UI controls)
            sources = ["NewsAPI"]  # Only NewsAPI
            time_range = 30  # Default to 30 days
            min_text_length = 50  # Default minimum text length
            sentiment_threshold = 0.05  # Default sentiment threshold
            max_items_per_source = 50  # Default max items
            
            # Action buttons
            st.markdown("---")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("Run Analysis", type="primary", use_container_width=True):
                    st.session_state.show_parameters = True
                    self.run_professional_analysis(sources, time_range, search_query, 
                                                 min_text_length, sentiment_threshold, max_items_per_source)
            
            with col2:
                if st.button("Clear Data", use_container_width=True):
                    st.session_state.analysis_data = pd.DataFrame()
                    st.session_state.show_parameters = False
                    st.rerun()
            
            # Status indicator
            st.markdown("---")
            st.subheader("Status")
            status_color = {
                "Ready": "üü¢",
                "Processing": "üü°", 
                "Complete": "üü¢",
                "Error": "üî¥"
            }
            st.write(f"{status_color.get(st.session_state.processing_status, '‚ö™')} {st.session_state.processing_status}")
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        return sources, time_range, search_query, min_text_length, sentiment_threshold, max_items_per_source
    
    def run_professional_analysis(self, sources, time_range, search_query, min_text_length, sentiment_threshold, max_items_per_source):
        """Run the complete professional analysis pipeline"""
        try:
            # Set processing state
            st.session_state.processing_status = "Processing"
            
            with st.spinner("Fetching data from sources..."):
                final_results = self.final_working_system.analyze_sentiment(search_query)
                
                if final_results and final_results.get('status') in ['SUCCESS', 'PARTIAL_SUCCESS']:
                    # Convert to DataFrame for compatibility
                    results_data = []
                    
                    # Get actual articles from final analysis
                    all_articles = final_results.get('articles', [])
                    sentiment_data = final_results.get('sentiment_analysis', {})
                    
                    for i, article in enumerate(all_articles):
                        # Use actual sentiment score if available
                        article_sentiment = 0.0
                        sentiment_data = final_results.get('sentiment_analysis', {})
                        if sentiment_data.get('overall_sentiment_score'):
                            article_sentiment = sentiment_data['overall_sentiment_score']
                            
                        results_data.append({
                            'timestamp': pd.to_datetime('now'),
                            'text': article.get('title', 'No title'),
                            'source': article.get('source', 'Unknown source'),
                            'url': article.get('url', ''),
                            'relevance_score': article.get('relevance_score', 0),
                            'sentiment': article_sentiment
                        })
                    
                    if results_data:
                        raw_data = pd.DataFrame(results_data)
                        all_dataframes = [raw_data]
                    else:
                        # Create minimal data for empty results
                        raw_data = pd.DataFrame([{
                            'timestamp': pd.to_datetime('now'),
                            'text': f"Analysis completed for {search_query}",
                            'source': 'final_working_system',
                            'relevance_score': 100,
                            'sentiment': 0.0
                        }])
                        all_dataframes = [raw_data]
                    
                    # Get results for animation state
                    domain_detection = final_results.get('domain_detection', {})
                    data_sources = final_results.get('data_sources', {})
                    sentiment_analysis = final_results.get('sentiment_analysis', {})
                    
                    total_articles = data_sources.get('total_articles', 0)
                    overall_sentiment = sentiment_analysis.get('overall_sentiment_label', 'NEUTRAL')
                
                else:
                    # Create minimal fallback data
                    raw_data = pd.DataFrame([{
                        'timestamp': pd.to_datetime('now'),
                        'text': f"Analysis attempted for {search_query}",
                        'source': 'final_working_system',
                        'relevance_score': 100,
                        'sentiment': 0.0
                    }])
                    all_dataframes = [raw_data]
            
            if not all_dataframes:
                st.warning("No data found for the specified parameters. Try adjusting your search terms or time range.")
                return
            
            # Create DataFrame and clean data
            with st.spinner("üßπ Cleaning and processing data..."):
                df = pd.concat(all_dataframes, ignore_index=True)
                
                # Simple data cleaning - remove duplicates and empty content
                # Check which columns exist and clean accordingly
                if 'title' in df.columns and 'description' in df.columns:
                    df = df.drop_duplicates(subset=['title', 'description'])
                    df = df.dropna(subset=['title', 'description'])
                elif 'text' in df.columns:
                    df = df.drop_duplicates(subset=['text'])
                    df = df.dropna(subset=['text'])
                else:
                    # Fallback: just remove empty rows
                    df = df.dropna()
                
                # Ensure we have a text column for analysis
                if 'text' not in df.columns:
                    if 'title' in df.columns and 'description' in df.columns:
                        df['text'] = df['title'].fillna('') + '. ' + df['description'].fillna('')
                    elif 'title' in df.columns:
                        df['text'] = df['title']
                    elif 'description' in df.columns:
                        df['text'] = df['description']
                
                # Filter by text length if text column exists
                if 'text' in df.columns:
                    df = df[df['text'].str.len() >= min_text_length]
            
            if df.empty:
                st.warning("No data remaining after cleaning. Try reducing the minimum text length.")
                return
            
            # Perform sentiment analysis
            with st.spinner("üéØ Analyzing sentiment..."):
                df = self.sentiment_analyzer.analyze_dataframe(df)
                df['text_length'] = df['text'].str.len()
            
            # Extract entities
            with st.spinner("üè∑Ô∏è Extracting entities..."):
                df = self.entity_extractor.extract_entities_dataframe(df)
            
            # Perform phrase analysis
            with st.spinner("üîç Analyzing key phrases..."):
                phrase_insights = self.phrase_analyzer.analyze_phrases(df['text'].tolist())
                # Extract key phrases list from the returned dictionary
                if isinstance(phrase_insights, dict) and 'key_phrases' in phrase_insights:
                    st.session_state.phrase_insights = phrase_insights['key_phrases']
                else:
                    st.session_state.phrase_insights = []
            
            # Store processed data
            st.session_state.analysis_data = df
            st.session_state.last_update = datetime.now()
            st.session_state.processing_status = "Complete"
            st.session_state.data_quality_score = create_professional_metrics(df)['data_quality_score']
            
            # Set success state if we have valid data
            if len(df) > 0:
                pass  # Analysis completed successfully
            else:
                pass  # No data found
            
        except Exception as e:
            logger.error(f"Error during analysis: {e}")
            st.session_state.processing_status = "Error"
            st.error(f"Analysis failed: {str(e)}")
    
    def render_professional_metrics(self):
        """Render professional metrics dashboard"""
        if st.session_state.analysis_data.empty:
            st.info("üëÜ Configure your analysis in the sidebar and click 'Run Analysis' to get started.")
            return
        
        df = st.session_state.analysis_data
        metrics = create_professional_metrics(df)
        
        # Display key metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="Total Items",
                value=f"{metrics['total_items']:,}",
                help="Total number of analyzed items"
            )
        
        with col2:
            sentiment_value = f"{metrics['avg_sentiment']:+.3f}"
            sentiment_delta = "Positive" if metrics['avg_sentiment'] > 0.05 else "Negative" if metrics['avg_sentiment'] < -0.05 else "Neutral"
            st.metric(
                label="Avg Sentiment", 
                value=sentiment_value,
                delta=sentiment_delta,
                help="Average sentiment score (-1 to +1)"
            )
        
        with col3:
            quality_color = "üü¢" if metrics['data_quality_score'] > 80 else "üü°" if metrics['data_quality_score'] > 60 else "üî¥"
            st.metric(
                label=f"{quality_color} Data Quality",
                value=f"{metrics['data_quality_score']:.0f}%",
                help="Data quality assessment"
            )
        
        with col4:
            st.metric(
                label="Source Diversity",
                value=f"{metrics['source_diversity']} sources",
                help="Number of different data sources"
            )
    
    def render_analysis_tabs(self):
        """Render the main analysis tabs"""
        if st.session_state.analysis_data.empty:
            return
        
        df = st.session_state.analysis_data
        
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìä Distribution", 
            "üîç Key Phrases",
            "üìã Sources",
            "üìÑ Data"
        ])
        
        with tab1:
            st.subheader("Distribution Analysis")
            dist_fig = self.visualizer.create_professional_distribution(df)
            st.plotly_chart(dist_fig, use_container_width=True)
        
        with tab2:
            st.subheader("Key Phrase Insights")
            if st.session_state.phrase_insights:
                # Display top phrases in a professional table
                phrase_fig = self.visualizer.create_professional_phrase_table(st.session_state.phrase_insights[:20])
                st.plotly_chart(phrase_fig, use_container_width=True)
                
                # Additional phrase analytics
                st.subheader("Top Topics")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Most Discussed:**")
                    top_phrases = st.session_state.phrase_insights[:5]
                    for i, phrase in enumerate(top_phrases, 1):
                        sentiment_emoji = "üòä" if phrase['avg_sentiment'] > 0.1 else "üòû" if phrase['avg_sentiment'] < -0.1 else "üòê"
                        st.write(f"{i}. {sentiment_emoji} **{phrase['phrase']}** ({phrase['frequency']} mentions)")
                
                with col2:
                    st.markdown("**Sentiment Distribution:**")
                    positive_phrases = sum(1 for p in st.session_state.phrase_insights[:10] if p['avg_sentiment'] > 0.1)
                    negative_phrases = sum(1 for p in st.session_state.phrase_insights[:10] if p['avg_sentiment'] < -0.1)
                    neutral_phrases = 10 - positive_phrases - negative_phrases
                    
                    st.write(f"‚úÖ Positive topics: {positive_phrases}")
                    st.write(f"‚ùå Negative topics: {negative_phrases}")
                    st.write(f"‚ö™ Neutral topics: {neutral_phrases}")
            else:
                st.info("No phrase insights available. Run the analysis to generate phrase data.")
        
        with tab3:
            st.subheader("Source Analysis")
            if 'source' in df.columns:
                source_fig = self.visualizer.create_source_analysis(df)
                st.plotly_chart(source_fig, use_container_width=True)
                
                # Source statistics
                source_stats = df.groupby('source').agg({
                    'compound': ['mean', 'count'],
                    'sentiment_label': lambda x: x.mode().iloc[0] if not x.empty else 'Neutral'
                }).round(3)
                
                st.subheader("Source Statistics")
                for source in df['source'].unique():
                    source_data = df[df['source'] == source]
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric(f"{source}", f"{len(source_data)} items")
                    with col2:
                        avg_sentiment = source_data['compound'].mean()
                        st.metric("Avg Sentiment", f"{avg_sentiment:+.3f}")
                    with col3:
                        dominant = source_data['sentiment_label'].mode().iloc[0] if not source_data.empty else 'N/A'
                        st.metric("Dominant", dominant)
            else:
                st.info("No source information available.")
        
        with tab4:
            st.subheader("Raw Data")
            
            # Data overview
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Records", len(df))
            with col2:
                st.metric("Columns", len(df.columns))
            with col3:
                memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024
                st.metric("Memory Usage", f"{memory_usage:.1f} MB")
            
            # Sample data
            st.subheader("Sample Data (First 100 rows)")
            display_df = df[['timestamp', 'source', 'text', 'compound', 'sentiment_label']].head(100)
            st.dataframe(display_df, width='stretch')
            
            # Download option
            if st.button("Download Dataset"):
                csv = df.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name=f"sentiment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
    
    def run(self):
        """Run the main application"""
        self.render_header()
        
        # Get configuration from sidebar
        sources, time_range, search_query, min_text_length, sentiment_threshold, max_items_per_source = self.render_professional_sidebar()
        
        # Display metrics
        self.render_professional_metrics()
        
        # Display analysis tabs
        self.render_analysis_tabs()
        
        # Footer
        st.markdown("---")
        st.markdown("""
        <div style='text-align: center; color: #666; font-size: 12px;'>
            üéØ Professional Sentiment Analysis Platform | Built with advanced NLP and real-time data integration
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    # Create and run the professional application
    app = ProfessionalSentimentApp()
    app.run()
